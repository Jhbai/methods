Paper: https://www.arxiv.org/pdf/2312.02530
---
# MEMTO: 基於記憶體增強 Transformer 的多元時間序列異常檢測

本專案實作了 MEMTO (Memory-Enhanced Transformer for Time-Series Anomaly Detection) 模型，專為多元時間序列的無監督異常檢測而設計。該模型結合了 Transformer 的序列特徵提取能力和一個獨特的門控記憶體模組，使其能夠學習並記住正常資料的典型模式，從而有效地識別偏離這些模式的異常點。

## 核心技術原理

MEMTO 的核心架構由三個主要部分組成：**編碼器 (Encoder)**、**門控記憶體模組 (Gated Memory Module)** 和 **解碼器 (Decoder)**。

1.  **Transformer 編碼器 (`encoder.py`)**
    *   **作用**：負責捕捉時間序列中的時間依賴性和特徵間的複雜關係。
    *   **細節**：模型首先將輸入的多元時間序列通過一個線性層投影到高維度的潛在空間。接著，加入位置編碼 (Positional Encoding) 以保留序列的時序資訊。最後，堆疊的 Transformer Encoder Layer 會處理這些嵌入向量，生成一系列能代表各時間點上下文資訊的 "查詢向量 (Queries)"。

2.  **門控記憶體模組 (`memory_gate.py`)**
    *   **作用**：此模組是 MEMTO 的關鍵。它學習並儲存了一組代表 "正常模式原型" 的記憶項 (Memory Items)。
    *   **細節**：
        *   **查詢更新 (Query Update)**：對於編碼器產生的每個查詢向量，記憶體模組會計算其與所有記憶體項的相似度 (Attention Score)，並根據相似度從記憶體中 "讀取" 最相關的模式原型。然後，將讀取到的記憶體資訊與原始查詢向量拼接，形成一個 "記憶體增強" 的新查詢向量。
        *   **記憶體更新 (Gated Memory Update)**：在訓練過程中，模型會使用一個門控機制 (Gating Mechanism) 來動態更新記憶體。這個機制會決定在多大程度上用當前的查詢向量去更新對應的記憶體項，從而讓記憶體不斷學習和適應訓練資料中的正常模式。

3.  **弱解碼器 (`decoder.py`)**
    *   **作用**：重建原始的時間序列信號。
    *   **細節**：解碼器接收來自記憶體模組的 "記憶體增強查詢向量"，並試圖將其還原成原始的輸入信號。它是一個相對簡單的多層感知機 (MLP)，其設計理念是，如果輸入的查詢向量代表的是正常模式，那麼經過記憶體增強後，解碼器應該能很好地重建它。反之，如果查詢向量代表異常模式，由於記憶體中沒有對應的正常原型，重建效果就會很差。

### 異常分數計算 (`main.py`)

模型的異常判斷基於兩個維度的偏差：

*   **輸入空間偏差 (ISD - Input Space Deviation)**：即重建誤差。計算原始時間序列與解碼器重建結果之間的歐幾里得距離。重建誤差越大，代表該時間點越可能是異常。
*   **潛在空間偏差 (LSD - Latent Space Deviation)**：計算編碼器產生的查詢向量與其在記憶體中最接近的模式原型之間的距離。如果一個查詢向量遠離所有已知的正常模式，代表它在潛在空間中是個離群點，也可能是異常。

最終的異常分數是這兩者的結合，透過 Softmax 權重進行加權，使其能同時考慮重建的保真度和模式的符合度。

## 模型訓練流程 (`training.py`)

為了穩定且有效地訓練模型，本專案採用了一個兩階段的訓練策略：

1.  **階段一：編碼器預訓練 (`phase1_initial_training`)**
    *   **目標**：讓編碼器和解碼器學習到一個初步但有效的特徵表示與重建能力。
    *   **方法**：在此階段，整個模型（包含記憶體模組）僅使用重建損失 (MSE Loss) 進行訓練。此時的目標很單純：讓模型學會如何重建輸入的正常時間序列。

2.  **階段二：使用 K-means 初始化記憶體 (`initialize_memory_with_kmeans`)**
    *   **目標**：為記憶體模組提供一個有意義的初始狀態，而不是隨機值。
    *   **方法**：在預訓練後，凍結模型，將一部分訓練資料通過編碼器生成查詢向量。接著，使用 K-means 演算法對這些查詢向量進行聚類，並將聚類中心 (Centroids) 作為記憶體模組的初始值。這確保了記憶體在訓練開始時就已經代表了資料在潛在空間中的主要分佈模式。

3.  **階段三：全模型訓練 (`phase2_full_training`)**
    *   **目標**：聯合優化整個模型，使其同時關注重建精度和記憶體使用的稀疏性。
    *   **方法**：在此階段，使用自定義的 `MEMTOLoss` 進行訓練。這個損失函數包含兩部分：
        *   **重建損失 (`L_rec`)**: 與階段一相同，衡量重建的精確度。
        *   **熵損失 (`L_entr`)**: 計算查詢與記憶體之間注意力權重的熵。此項的目的是鼓勵模型使用更多樣化的記憶體項，而不是將所有注意力集中在少數幾個記憶體上，避免模型退化。

## 專案結構

```
.
├── main.py             # 主執行檔：包含資料生成、模型訓練、推論和視覺化的完整範例
├── memto.py            # MEMTO 模型主結構，整合 Encoder, Memory, Decoder
├── encoder.py          # Transformer 編碼器模組
├── memory_gate.py      # 門控記憶體模組
├── decoder.py          # 弱解碼器模組
├── loss.py             # 自定義的 MEMTOLoss 損失函數
└── training.py         # 包含兩個訓練階段和 K-means 初始化功能的函式
```

## 使用方法

### 1. 環境設置

請先安裝必要的 Python 函式庫。建議在虛擬環境中進行安裝。

```bash
pip install torch numpy scikit-learn matplotlib
```

如果您的環境支援 CUDA，請確保安裝與您的 CUDA 版本相容的 PyTorch。

### 2. 執行訓練與推論

本專案的 `main.py` 是一個端到端的範例，包含了從資料生成到結果視覺化的所有步驟。直接執行此檔案即可開始。

```bash
python main.py
```

程式將會依序執行以下步驟：
1.  **生成資料**: 產生用於訓練的純淨時間序列資料。
2.  **模型初始化**: 建立 MEMTO 模型、損失函數和優化器。
3.  **訓練階段一**: 進行編碼器預訓練。
4.  **K-means 初始化**: 初始化記憶體模組。
5.  **訓練階段二**: 使用完整的損失函數進行全模型訓練。
6.  **推論與視覺化**:
    *   生成一份帶有異常點的測試資料。
    *   選擇其中一個樣本進行推論。
    *   計算異常分數。
    *   繪製結果圖，包含原始序列、重建序列和異常分數。

### 3. 客製化

您可以在 `main.py` 的開頭輕鬆調整模型的超參數和訓練設置：

```python
# ----- Training ----- #
# Init parameters
N_SAMPLES = 500       # 訓練樣本數
N_TIMESTEPS = 100     # 每個樣本的時間步長
...

# Model hyperparameters
LATENT_DIM = 64         # 潛在空間維度
N_HEADS = 4             # Transformer 注意力頭數
NUM_MEMORY_ITEMS = 10   # 記憶體大小
...

# Training hyperparameters
LR = 1e-4               # 學習率
PHASE1_EPOCHS = 3       # 階段一訓練輪數
PHASE2_EPOCHS = 5       # 階段二訓練輪數
LAMBDA_ENTROPY = 0.01   # 熵損失的權重
```

## 結果視覺化

執行 `main.py` 後，將會顯示一張結果圖，如下所示：

*   **頂部三個子圖**: 分別展示了三個特徵維度的原始序列（藍色實線）和模型的重建序列（紅色虛線）。橘色背景區域標示了真實的異常位置。
*   **底部子圖**: 顯示了模型計算出的最終異常分數（綠色線）。分數的峰值應對應到真實的異常區域。

這張圖可以直觀地評估模型在特定樣本上的表現。
