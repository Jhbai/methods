### 1. 核心目標：解決「過度泛化 (Over-generalization)」問題

在傳統的基於重建的異常檢測模型（如 Autoencoder）中，模型訓練的目標是最小化輸入和輸出之間的重建誤差。但這會引發一個問題：如果模型能力太強（例如，解碼器太複雜），它會學會**重建任何輸入**，無論是正常還是異常數據。

這就導致了「過度泛化」：
*   **正常數據**：輸入 -> 編碼 -> 解碼 -> **完美重建** (重建誤差低)
*   **異常數據**：輸入 -> 編碼 -> 解碼 -> **也完美重建** (重建誤差也很低)

當正常和異常數據的重建誤差都很低時，模型就失去了區分它們的能力，異常檢測任務便失敗了。

### 2. 核心思想：建立一個「正常模式的原型庫」

MEMTO 的 Memory Module 就像是為模型建立了一個**只能查詢「正常」數據模式的原型庫 (Prototype Library) 或字典**。它強迫模型的重建過程必須經過這個「正常庫」的篩選和參考，而不是直接從原始的編碼資訊中重建。

這個設計創造了一個**重建瓶頸 (Reconstruction Bottleneck)**：
*   **對於正常數據**：其編碼（稱為查詢 Query）能在 Memory 中找到高度匹配的正常原型。結合原始查詢和匹配的原型，模型可以很好地重建它，誤差很低。
*   **對於異常數據**：其查詢在 Memory 中**找不到**匹配的正常原型。模型被迫使用一些「不太相關」的正常原型來輔助重建。這會導致重建結果**更像一個「正常」的樣本，而不是原始的「異常」樣本**，從而使得重建輸出與異常輸入之間的**誤差變得非常大**。

通過放大異常數據的重建誤差，模型就能有效地將其檢測出來。

### 3. 運作機制：兩個獨立但互補的更新階段

Memory Module 的運作可以分為兩個階段：**查詢更新 (Query Update)** 和 **門控記憶體更新 (Gated Memory Update)**。

#### A. 查詢更新 (Query Update Stage) - 如何「使用」記憶體

這個階段在**訓練和推論時**都會發生，其目標是生成最終用於解碼的特徵。

1.  **計算相似度**：輸入的查詢序列 `q`（來自 Transformer Encoder）與 Memory 中的每一個原型 `m` 進行點積運算，再通過 Softmax 得到注意力權重 `w`。這個權重 `w` 代表了**每個時間點的輸入與哪個正常原型最相似**。
2.  **檢索記憶體**：利用注意力權重 `w` 對所有記憶體原型 `m` 進行加權求和，得到一個「檢索後的記憶體」`~q`。這個 `~q` 是根據當前輸入動態合成的、最相關的正常模式。
3.  **拼接特徵**：將原始的查詢 `q` 和檢索到的記憶體 `~q` 在特徵維度上拼接 (concatenate) 起來，形成一個維度為 `2*C` 的「更新後查詢」(Updated Query)。
4.  **送入解碼器**：這個拼接後的特徵被送入 Weak Decoder 進行最終的重建。

**直觀理解**：這個過程確保了解碼器看到的不是原始輸入的全部資訊，而是一個**混合體**：一半是原始資訊，一半是從「正常庫」中找到的最相關的模式。

#### B. 門控記憶體更新 (Gated Memory Update Stage) - 如何「學習」記憶體

這個階段**僅在訓練時**發生，其目標是讓 Memory Module **動態地、自適應地學習和更新**正常數據的原型。

1.  **計算更新門控 `ψ`**：這是最關鍵的一步。模型會根據當前的輸入查詢 `q` 和現有的記憶體 `m`，通過兩個線性層 (`Uψ` 和 `Wψ`) 和一個 Sigmoid 函數，學習一個**更新門控值 `ψ`**（介於 0 和 1 之間）。
2.  **控制更新程度**：這個門控 `ψ` 決定了每個記憶體原型應該在多大程度上被新的輸入數據所更新。
    *   如果 `ψ` 接近 0，表示「保持舊有記憶，新資訊不重要」，記憶體幾乎不變。
    *   如果 `ψ` 接近 1，表示「新資訊很重要，用它來大幅更新記憶」，記憶體會變得更像新的輸入數據。
3.  **執行更新**：最終的記憶體更新公式為 `m_new = (1 - ψ) * m_old + ψ * update_info`。這是一個非常經典的門控更新機制（類似於 LSTM 或 GRU 中的門控），它允許模型以**數據驅動 (data-driven)** 的方式決定如何維護這個「正常庫」，而不是簡單地進行平均。

**直觀理解**：這個機制讓 Memory Module 變得非常靈活。它不是死板地記住幾個模式，而是能夠在訓練過程中，根據數據的分佈動態調整原型，使其能更好地代表所有「正常」的行為模式。

### 4. 兩階段訓練範式 (Two-phase Training) 的輔助

由於門控更新是增量式的，如果一開始 Memory 中的原型是隨機初始化的「噪聲」，整個訓練過程可能會不穩定。因此，論文提出：
1.  **第一階段**：先只用重建損失訓練模型，目標是得到一個好的 Encoder，能將時間序列映射到有意義的特徵空間。
2.  **K-means 初始化**：使用訓練好的 Encoder 提取所有訓練數據的特徵（查詢），然後用 **K-means 演算法**對這些特徵進行分群。**每個分群的中心點 (centroid) 就被用來作為 Memory Module 的初始原型**。這提供了一個非常穩定且有意義的起點。
3.  **第二階段**：用初始化的 Memory 和完整的損失函數（重建損失 + 熵損失）來訓練整個模型。

### 總結

| 機制 | **查詢更新 (Query Update)** | **門控記憶體更新 (Gated Memory Update)** |
| :--- | :--- | :--- |
| **目的** | **使用**記憶體進行重建 | **學習**和**調整**記憶體原型 |
| **執行時機** | 訓練 & 推論 | **僅在訓練時** |
| **核心** | 拼接原始查詢和檢索到的正常模式 | 學習一個**門控 `ψ`** 來控制更新強度 |
| **作用** | 強制重建過程參考正常模式，放大異常誤差 | 使記憶體能**自適應地**捕捉數據中的多樣正常模式 |

您可以將 MEMTO 的 Memory Module 想像成一個**由一群不斷學習的專家組成的評審團**。當一個新的數據點（查詢）進來時：
1.  **查詢更新**：數據點需要向評審團報告，評審團會根據與其最相似的專家意見，共同給出一個「符合規範」的建議。最終的決策（重建）必須基於原始報告和這個「規範建議」。
2.  **門控記憶體更新**：在內部培訓時，如果一個新的正常案例很有代表性，專家們會通過一個「門控機制」來決定是否以及在多大程度上更新自己的知識庫，以保持其專業性。
